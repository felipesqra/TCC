{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analisa Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (71.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\felipe\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\felipe\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\felipe\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
      "     ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/13.0 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/13.0 MB 110.1 kB/s eta 0:01:58\n",
      "     --------------------------------------- 0.0/13.0 MB 110.1 kB/s eta 0:01:58\n",
      "     --------------------------------------- 0.0/13.0 MB 164.3 kB/s eta 0:01:19\n",
      "     --------------------------------------- 0.1/13.0 MB 218.8 kB/s eta 0:01:00\n",
      "      -------------------------------------- 0.2/13.0 MB 619.5 kB/s eta 0:00:21\n",
      "     - -------------------------------------- 0.6/13.0 MB 1.7 MB/s eta 0:00:08\n",
      "     -- ------------------------------------- 1.0/13.0 MB 2.5 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/13.0 MB 3.2 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/13.0 MB 3.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/13.0 MB 4.1 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.5/13.0 MB 4.4 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.7/13.0 MB 4.5 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.1/13.0 MB 4.7 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.5/13.0 MB 4.9 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.9/13.0 MB 5.2 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 4.3/13.0 MB 5.4 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.5/13.0 MB 5.5 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 5.0/13.0 MB 5.6 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.4/13.0 MB 5.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.8/13.0 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.2/13.0 MB 6.0 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.6/13.0 MB 6.1 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/13.0 MB 6.2 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.4/13.0 MB 6.3 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.7/13.0 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 8.1/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.5/13.0 MB 6.4 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.9/13.0 MB 6.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/13.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.6/13.0 MB 6.6 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 10.0/13.0 MB 6.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.3/13.0 MB 8.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.8/13.0 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.2/13.0 MB 8.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/13.0 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.9/13.0 MB 8.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.2/13.0 MB 8.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.6/13.0 MB 8.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  13.0/13.0 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  13.0/13.0 MB 8.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 13.0/13.0 MB 7.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pt-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (71.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\felipe\\appdata\\roaming\\python\\python312\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\felipe\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\felipe\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.22.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.26.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client>=1.8.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\felipe\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "!pip install selenium\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrai texto dos links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def extract_text_from_links(links):\n",
    "    # Configure Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run headless Chrome\n",
    "\n",
    "    # Initialize ChromeDriver\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    extracted_texts = []\n",
    "\n",
    "    for link in links:\n",
    "        try:\n",
    "            driver.get(link)\n",
    "            \n",
    "            # Find the main container\n",
    "            main_div = driver.find_element(By.CSS_SELECTOR, 'div.flex.flex-col.text-sm.pt-2.md\\\\:pb-9')\n",
    "            \n",
    "            # Remove the div we want to ignore\n",
    "            try:\n",
    "                ignored_div = main_div.find_element(By.CSS_SELECTOR, 'div.mx-auto.flex.flex-1.gap-4.text-base.md\\\\:gap-5.lg\\\\:gap-6.md\\\\:max-w-3xl.lg\\\\:max-w-\\\\[40rem\\\\].xl\\\\:max-w-\\\\[48rem\\\\]')\n",
    "                driver.execute_script(\"\"\"\n",
    "                    var element = arguments[0];\n",
    "                    element.parentNode.removeChild(element);\n",
    "                    \"\"\", ignored_div)\n",
    "            except:\n",
    "                pass  # Ignore if the element is not found\n",
    "            \n",
    "            # Extract text from the remaining content\n",
    "            extracted_text = main_div.text\n",
    "            extracted_texts.append(extracted_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {link}: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return extracted_texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estou realizando um experimento, alunos vão interagir com o chatgpt, o texto da interação deles será exportado e analisado. O aluno irá utilizar o chatgpt para tentar resolver problemas de ciência de dados descritiva, a questão diz respeito a insights obtidos sobre uma base de dados, quero um algoritmo python de PLN que analise quais são os insights gerados pelo chatgpt neste texto, por exemplo, no texto gerado pelo chatgpt abaixo ele sugere 8 insights:\n",
      "\n",
      "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
      "Entity\n",
      "Date\n",
      "Seasonal variation\n",
      "Combined measurements\n",
      "Monthly averaged\n",
      "Annual averaged\n",
      "sea_temperature_anomaly\n",
      "Sea surface temp (lower-bound)\n",
      "Sea surface temp (upper-bound)\n",
      "Monthly pH measurement\n",
      "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
      "\n",
      "Análises Temporais\n",
      "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
      "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
      "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
      "Análises de Anomalias\n",
      "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
      "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
      "Análises Estatísticas\n",
      "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
      "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
      "Análises Comparativas\n",
      "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
      "Para criar um algoritmo em Python que analise textos gerados pelo ChatGPT e identifique insights, você pode usar técnicas de Processamento de Linguagem Natural (PLN) como a tokenização, identificação de palavras-chave e análise de padrões. O exemplo abaixo demonstra como você pode implementar isso usando a biblioteca spaCy e técnicas de PLN.\n",
      "Primeiro, você precisará instalar a biblioteca spaCy e baixar o modelo de idioma:\n",
      "bash\n",
      "Copiar código\n",
      "pip install spacy\n",
      "python -m spacy download pt_core_news_sm\n",
      "Em seguida, você pode usar o seguinte código Python para analisar os textos:\n",
      "python\n",
      "Copiar código\n",
      "import spacy\n",
      "\n",
      "# Carregar o modelo de idioma em português\n",
      "nlp = spacy.load('pt_core_news_sm')\n",
      "\n",
      "# Função para extrair insights do texto\n",
      "def extrair_insights(texto):\n",
      "    # Processar o texto com spaCy\n",
      "    doc = nlp(texto)\n",
      "    \n",
      "    # Definir categorias de insights que queremos identificar\n",
      "    categorias = {\n",
      "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
      "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
      "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
      "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
      "    }\n",
      "    \n",
      "    # Armazenar insights encontrados\n",
      "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
      "    \n",
      "    # Percorrer as sentenças do documento\n",
      "    for sent in doc.sents:\n",
      "        # Verificar se a sentença corresponde a algum insight nas categorias\n",
      "        for categoria, insights in categorias.items():\n",
      "            for insight in insights:\n",
      "                if insight.lower() in sent.text.lower():\n",
      "                    insights_encontrados[categoria].append(sent.text.strip())\n",
      "                    break\n",
      "    \n",
      "    return insights_encontrados\n",
      "\n",
      "# Texto de exemplo fornecido pelo usuário\n",
      "texto = \"\"\"\n",
      "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
      "Entity\n",
      "Date\n",
      "Seasonal variation\n",
      "Combined measurements\n",
      "Monthly averaged\n",
      "Annual averaged\n",
      "sea_temperature_anomaly\n",
      "Sea surface temp (lower-bound)\n",
      "Sea surface temp (upper-bound)\n",
      "Monthly pH measurement\n",
      "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
      "\n",
      "Análises Temporais\n",
      "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
      "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
      "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
      "Análises de Anomalias\n",
      "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
      "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
      "Análises Estatísticas\n",
      "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
      "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
      "Análises Comparativas\n",
      "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
      "\"\"\"\n",
      "\n",
      "# Extrair e exibir os insights do texto\n",
      "insights = extrair_insights(texto)\n",
      "for categoria, sentencas in insights.items():\n",
      "    print(f\"{categoria}:\")\n",
      "    for sentenca in sentencas:\n",
      "        print(f\" - {sentenca}\")\n",
      "Explicação:\n",
      "Instalação de spaCy e download do modelo: Você precisa instalar a biblioteca spaCy e baixar o modelo de idioma português (pt_core_news_sm).\n",
      "Definição das categorias de insights: Criei um dicionário categorias que mapeia as diferentes categorias de análises e seus insights correspondentes.\n",
      "Processamento do texto: A função extrair_insights processa o texto usando spaCy, identifica sentenças que correspondem a cada insight e armazena os resultados.\n",
      "Extração e exibição dos insights: O script percorre o texto de exemplo fornecido e exibe os insights encontrados em cada categoria.\n",
      "Esse algoritmo pode ser expandido e adaptado conforme necessário para melhor atender às especificidades do seu experimento e análise.\n",
      "explique melhor o codigo que você gerou\n",
      "Claro, vou explicar detalhadamente cada parte do código e seu funcionamento.\n",
      "Instalação de dependências e download do modelo de idioma\n",
      "Primeiro, precisamos instalar a biblioteca spaCy e baixar o modelo de idioma português:\n",
      "bash\n",
      "Copiar código\n",
      "pip install spacy\n",
      "python -m spacy download pt_core_news_sm\n",
      "Importação da biblioteca e carregamento do modelo\n",
      "python\n",
      "Copiar código\n",
      "import spacy\n",
      "\n",
      "# Carregar o modelo de idioma em português\n",
      "nlp = spacy.load('pt_core_news_sm')\n",
      "spacy: Importa a biblioteca spaCy que será utilizada para o processamento de linguagem natural.\n",
      "nlp: Carrega o modelo de idioma em português (pt_core_news_sm) que é necessário para processar o texto em português.\n",
      "Função para extrair insights do texto\n",
      "python\n",
      "Copiar código\n",
      "def extrair_insights(texto):\n",
      "    # Processar o texto com spaCy\n",
      "    doc = nlp(texto)\n",
      "    \n",
      "    # Definir categorias de insights que queremos identificar\n",
      "    categorias = {\n",
      "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
      "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
      "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
      "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
      "    }\n",
      "    \n",
      "    # Armazenar insights encontrados\n",
      "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
      "    \n",
      "    # Percorrer as sentenças do documento\n",
      "    for sent in doc.sents:\n",
      "        # Verificar se a sentença corresponde a algum insight nas categorias\n",
      "        for categoria, insights in categorias.items():\n",
      "            for insight in insights:\n",
      "                if insight.lower() in sent.text.lower():\n",
      "                    insights_encontrados[categoria].append(sent.text.strip())\n",
      "                    break\n",
      "    \n",
      "    return insights_encontrados\n",
      "Processamento do texto com spaCy:\n",
      "doc = nlp(texto): Processa o texto fornecido usando o modelo nlp. Isso converte o texto em um objeto Doc que pode ser analisado em termos de sentenças, tokens e outros componentes linguísticos.\n",
      "Definição das categorias de insights:\n",
      "categorias: Um dicionário que mapeia categorias de análises para listas de possíveis insights que queremos identificar no texto. Cada chave é uma categoria e cada valor é uma lista de possíveis frases de insights.\n",
      "Inicialização da estrutura para armazenar os insights encontrados:\n",
      "insights_encontrados = {categoria: [] for categoria in categorias}: Inicializa um dicionário onde cada chave é uma categoria e cada valor é uma lista vazia que armazenará os insights encontrados.\n",
      "Percorrer as sentenças do documento:\n",
      "for sent in doc.sents: Itera sobre cada sentença do documento processado.\n",
      "Verificar se a sentença corresponde a algum insight nas categorias:\n",
      "Para cada sentença, verificamos se contém alguma das frases de insight definidas em categorias.\n",
      "if insight.lower() in sent.text.lower(): Verifica se a frase de insight aparece na sentença, ignorando diferenças de capitalização (letras maiúsculas e minúsculas).\n",
      "insights_encontrados[categoria].append(sent.text.strip()): Se a frase de insight é encontrada na sentença, a sentença é adicionada à lista correspondente no dicionário insights_encontrados.\n",
      "Retorna os insights encontrados:\n",
      "return insights_encontrados: A função retorna o dicionário insights_encontrados contendo as sentenças de insights agrupadas por categoria.\n",
      "Texto de exemplo fornecido pelo usuário\n",
      "python\n",
      "Copiar código\n",
      "# Texto de exemplo fornecido pelo usuário\n",
      "texto = \"\"\"\n",
      "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
      "Entity\n",
      "Date\n",
      "Seasonal variation\n",
      "Combined measurements\n",
      "Monthly averaged\n",
      "Annual averaged\n",
      "sea_temperature_anomaly\n",
      "Sea surface temp (lower-bound)\n",
      "Sea surface temp (upper-bound)\n",
      "Monthly pH measurement\n",
      "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
      "\n",
      "Análises Temporais\n",
      "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
      "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
      "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
      "Análises de Anomalias\n",
      "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
      "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
      "Análises Estatísticas\n",
      "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
      "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
      "Análises Comparativas\n",
      "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
      "\"\"\"\n",
      "Extração e exibição dos insights\n",
      "python\n",
      "Copiar código\n",
      "# Extrair e exibir os insights do texto\n",
      "insights = extrair_insights(texto)\n",
      "for categoria, sentencas in insights.items():\n",
      "    print(f\"{categoria}:\")\n",
      "    for sentenca in sentencas:\n",
      "        print(f\" - {sentenca}\")\n",
      "Extrair insights do texto:\n",
      "insights = extrair_insights(texto): Chama a função extrair_insights com o texto de exemplo e armazena o resultado no dicionário insights.\n",
      "Exibir os insights extraídos:\n",
      "for categoria, sentencas in insights.items(): Itera sobre cada categoria de insights e suas sentenças correspondentes.\n",
      "print(f\"{categoria}:\"): Imprime o nome da categoria.\n",
      "for sentenca in sentencas: Itera sobre cada sentença de insight na categoria.\n",
      "print(f\" - {sentenca}\"): Imprime cada sentença de insight, precedida por um traço para facilitar a leitura.\n",
      "Resumo\n",
      "O código processa um texto usando técnicas de PLN para identificar e extrair insights específicos definidos em categorias predefinidas. Ele utiliza o modelo de linguagem spaCy para análise e processamento, verifica a presença de frases de insight no texto e organiza os resultados por categoria. A abordagem é modular e pode ser adaptada para diferentes contextos e necessidades de análise.\n",
      "lembre-se que o código sera utilizado em um notebook no google colab\n",
      "Entendi, vamos adaptar o código para que funcione de forma eficaz no Google Colab. Aqui está o código completo, incluindo a instalação das dependências diretamente no notebook:\n",
      "Passo 1: Instalação das Dependências\n",
      "No Google Colab, você pode instalar as dependências diretamente em uma célula do notebook:\n",
      "python\n",
      "Copiar código\n",
      "!pip install spacy\n",
      "!python -m spacy download pt_core_news_sm\n",
      "Passo 2: Código Completo para Análise de Insights\n",
      "Em seguida, você pode criar uma célula com o código para analisar os textos e extrair insights:\n",
      "python\n",
      "Copiar código\n",
      "import spacy\n",
      "\n",
      "# Carregar o modelo de idioma em português\n",
      "nlp = spacy.load('pt_core_news_sm')\n",
      "\n",
      "# Função para extrair insights do texto\n",
      "def extrair_insights(texto):\n",
      "    # Processar o texto com spaCy\n",
      "    doc = nlp(texto)\n",
      "    \n",
      "    # Definir categorias de insights que queremos identificar\n",
      "    categorias = {\n",
      "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
      "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
      "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
      "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
      "    }\n",
      "    \n",
      "    # Armazenar insights encontrados\n",
      "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
      "    \n",
      "    # Percorrer as sentenças do documento\n",
      "    for sent in doc.sents:\n",
      "        # Verificar se a sentença corresponde a algum insight nas categorias\n",
      "        for categoria, insights in categorias.items():\n",
      "            for insight in insights:\n",
      "                if insight.lower() in sent.text.lower():\n",
      "                    insights_encontrados[categoria].append(sent.text.strip())\n",
      "                    break\n",
      "    \n",
      "    return insights_encontrados\n",
      "\n",
      "# Texto de exemplo fornecido pelo usuário\n",
      "texto = \"\"\"\n",
      "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
      "Entity\n",
      "Date\n",
      "Seasonal variation\n",
      "Combined measurements\n",
      "Monthly averaged\n",
      "Annual averaged\n",
      "sea_temperature_anomaly\n",
      "Sea surface temp (lower-bound)\n",
      "Sea surface temp (upper-bound)\n",
      "Monthly pH measurement\n",
      "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
      "\n",
      "Análises Temporais\n",
      "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
      "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
      "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
      "Análises de Anomalias\n",
      "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
      "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
      "Análises Estatísticas\n",
      "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
      "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
      "Análises Comparativas\n",
      "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
      "\"\"\"\n",
      "\n",
      "# Extrair e exibir os insights do texto\n",
      "insights = extrair_insights(texto)\n",
      "for categoria, sentencas in insights.items():\n",
      "    print(f\"{categoria}:\")\n",
      "    for sentenca in sentencas:\n",
      "        print(f\" - {sentenca}\")\n",
      "Explicação\n",
      "Instalação das Dependências:\n",
      "As dependências são instaladas diretamente em uma célula do Google Colab usando !pip install spacy e !python -m spacy download pt_core_news_sm.\n",
      "Importação e Carregamento do Modelo:\n",
      "O modelo de linguagem spaCy é carregado com nlp = spacy.load('pt_core_news_sm').\n",
      "Definição da Função extrair_insights:\n",
      "A função extrair_insights processa o texto e verifica a presença de frases de insight predefinidas, organizando os resultados por categoria.\n",
      "Texto de Exemplo e Extração de Insights:\n",
      "Um texto de exemplo é fornecido e a função extrair_insights é chamada para analisar o texto e extrair os insights.\n",
      "Os insights são então exibidos por categoria.\n",
      "Este código pode ser executado em um notebook do Google Colab e permite que você analise textos para extrair insights específicos definidos nas categorias de análise.\n",
      "explique melhor a biblioteca spacy\n",
      "Claro! Vamos explorar a biblioteca spaCy e entender como ela funciona, incluindo suas principais funcionalidades e como usá-la para Processamento de Linguagem Natural (PLN).\n",
      "O que é spaCy?\n",
      "spaCy é uma biblioteca de código aberto para processamento de linguagem natural (PLN) avançado em Python. Ela é projetada para ser rápida, eficiente e capaz de lidar com grandes volumes de texto. spaCy fornece ferramentas para várias tarefas de PLN, como tokenização, análise morfológica, extração de entidades nomeadas e muito mais.\n",
      "Funcionalidades Principais de spaCy\n",
      "Tokenização: Dividir o texto em palavras, pontuações e outros elementos chamados \"tokens\".\n",
      "POS Tagging (Part-of-Speech Tagging): Identificar a função gramatical de cada token (substantivo, verbo, adjetivo, etc.).\n",
      "Parsing: Analisar a estrutura gramatical do texto, identificando relações entre as palavras.\n",
      "NER (Named Entity Recognition): Identificar e classificar entidades nomeadas (pessoas, organizações, locais, etc.) no texto.\n",
      "Vetores de Palavra (Word Vectors): Representar palavras como vetores de números para análise semântica.\n",
      "Lemmatização: Reduzir palavras às suas formas básicas ou radicais.\n",
      "Similaridade de Texto: Calcular a similaridade entre diferentes textos.\n",
      "Como usar spaCy\n",
      "Instalação\n",
      "Para instalar spaCy e um modelo de idioma, você pode usar os seguintes comandos:\n",
      "bash\n",
      "Copiar código\n",
      "pip install spacy\n",
      "python -m spacy download pt_core_news_sm\n",
      "Carregamento de um Modelo de Idioma\n",
      "Após a instalação, você pode carregar um modelo de idioma:\n",
      "python\n",
      "Copiar código\n",
      "import spacy\n",
      "\n",
      "# Carregar o modelo de idioma em português\n",
      "nlp = spacy.load('pt_core_news_sm')\n",
      "Processamento de Texto\n",
      "Com o modelo carregado, você pode processar texto:\n",
      "python\n",
      "Copiar código\n",
      "# Texto de exemplo\n",
      "texto = \"A análise de dados é essencial para entender padrões climáticos.\"\n",
      "\n",
      "# Processar o texto\n",
      "doc = nlp(texto)\n",
      "\n",
      "# Iterar sobre tokens no documento\n",
      "for token in doc:\n",
      "    print(token.text, token.pos_, token.dep_)\n",
      "token.text: O texto do token.\n",
      "token.pos_: A parte do discurso (part-of-speech) do token.\n",
      "token.dep_: A relação de dependência gramatical do token.\n",
      "Extração de Entidades Nomeadas\n",
      "python\n",
      "Copiar código\n",
      "# Texto de exemplo com entidades nomeadas\n",
      "texto = \"A análise de dados realizada pela Universidade de São Paulo é essencial.\"\n",
      "\n",
      "# Processar o texto\n",
      "doc = nlp(texto)\n",
      "\n",
      "# Iterar sobre entidades no documento\n",
      "for ent in doc.ents:\n",
      "    print(ent.text, ent.label_)\n",
      "ent.text: O texto da entidade nomeada.\n",
      "ent.label_: O rótulo da entidade (e.g., organização, pessoa, localização).\n",
      "Similaridade de Texto\n",
      "Você pode comparar a similaridade entre documentos ou frases:\n",
      "python\n",
      "Copiar código\n",
      "doc1 = nlp(\"A análise de dados é essencial.\")\n",
      "doc2 = nlp(\"A estatística é uma ferramenta fundamental para a ciência de dados.\")\n",
      "\n",
      "# Calcular similaridade\n",
      "similaridade = doc1.similarity(doc2)\n",
      "print(f\"Similaridade: {similaridade}\")\n",
      "Aplicação: Extração de Insights\n",
      "Usando as funcionalidades de spaCy, você pode criar funções para tarefas específicas, como a extração de insights que mostramos anteriormente. spaCy permite analisar texto de forma detalhada e extrair informações relevantes com facilidade.\n",
      "Exemplo Completo no Google Colab\n",
      "python\n",
      "Copiar código\n",
      "# Passo 1: Instalação das Dependências\n",
      "!pip install spacy\n",
      "!python -m spacy download pt_core_news_sm\n",
      "\n",
      "# Passo 2: Importar e Carregar o Modelo\n",
      "import spacy\n",
      "\n",
      "nlp = spacy.load('pt_core_news_sm')\n",
      "\n",
      "# Função para extrair insights do texto\n",
      "def extrair_insights(texto):\n",
      "    doc = nlp(texto)\n",
      "    categorias = {\n",
      "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
      "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
      "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
      "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
      "    }\n",
      "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
      "    \n",
      "    for sent in doc.sents:\n",
      "        for categoria, insights in categorias.items():\n",
      "            for insight in insights:\n",
      "                if insight.lower() in sent.text.lower():\n",
      "                    insights_encontrados[categoria].append(sent.text.strip())\n",
      "                    break\n",
      "    \n",
      "    return insights_encontrados\n",
      "\n",
      "# Texto de exemplo fornecido pelo usuário\n",
      "texto = \"\"\"\n",
      "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
      "Entity\n",
      "Date\n",
      "Seasonal variation\n",
      "Combined measurements\n",
      "Monthly averaged\n",
      "Annual averaged\n",
      "sea_temperature_anomaly\n",
      "Sea surface temp (lower-bound)\n",
      "Sea surface temp (upper-bound)\n",
      "Monthly pH measurement\n",
      "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
      "\n",
      "Análises Temporais\n",
      "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
      "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
      "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
      "Análises de Anomalias\n",
      "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
      "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
      "Análises Estatísticas\n",
      "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
      "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
      "Análises Comparativas\n",
      "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
      "\"\"\"\n",
      "\n",
      "# Extrair e exibir os insights do texto\n",
      "insights = extrair_insights(texto)\n",
      "for categoria, sentencas in insights.items():\n",
      "    print(f\"{categoria}:\")\n",
      "    for sentenca in sentencas:\n",
      "        print(f\" - {sentenca}\")\n",
      "Conclusão\n",
      "A biblioteca spaCy é uma ferramenta poderosa para PLN, permitindo análise e processamento avançados de texto. Com suas diversas funcionalidades, spaCy facilita a extração de insights e informações relevantes de grandes volumes de texto, sendo uma excelente escolha para projetos de ciência de dados e análise de texto.\n",
      "eu quero que o algoritmo consiga identificar quais as falas geradas por alunos e quais as respostas fornecidas pelo chatgpt, por exemplo no texto abaixo existem marcações de onde a fala do chatgpt começa, mas não existe indicação de onde ela termina:\n",
      "\n",
      "oi\n",
      "ChatGPT\n",
      "Olá! Como posso ajudar hoje?\n",
      "\n",
      "bom dia\n",
      "ChatGPT\n",
      "Bom dia! Como está hoje? Posso te ajudar com alguma coisa?\n",
      "Para criar um algoritmo que consiga identificar quais são as falas geradas por alunos e quais são as respostas fornecidas pelo ChatGPT, podemos usar a estrutura do texto e as marcações fornecidas. Vamos desenvolver um código que faça essa separação.\n",
      "Passo 1: Instalação das Dependências\n",
      "Primeiro, vamos garantir que temos o spaCy instalado e o modelo de idioma em português baixado, caso ainda não tenha sido feito:\n",
      "bash\n",
      "Copiar código\n",
      "!pip install spacy\n",
      "!python -m spacy download pt_core_news_sm\n",
      "Passo 2: Função para Separar Falas\n",
      "Vamos criar uma função que separa as falas dos alunos e as respostas do ChatGPT:\n",
      "python\n",
      "Copiar código\n",
      "import spacy\n",
      "\n",
      "# Carregar o modelo de idioma em português\n",
      "nlp = spacy.load('pt_core_news_sm')\n",
      "\n",
      "# Função para separar falas de alunos e respostas do ChatGPT\n",
      "def separar_falas(texto):\n",
      "    # Inicializar listas para armazenar falas de alunos e respostas do ChatGPT\n",
      "    falas_alunos = []\n",
      "    respostas_chatgpt = []\n",
      "    \n",
      "    # Dividir o texto por linhas\n",
      "    linhas = texto.split('\\n')\n",
      "    \n",
      "    # Variável para controlar se estamos lendo uma resposta do ChatGPT\n",
      "    lendo_resposta = False\n",
      "    resposta_atual = \"\"\n",
      "    \n",
      "    for linha in linhas:\n",
      "        linha = linha.strip()\n",
      "        if linha == \"ChatGPT\":\n",
      "            if lendo_resposta and resposta_atual:\n",
      "                respostas_chatgpt.append(resposta_atual.strip())\n",
      "                resposta_atual = \"\"\n",
      "            lendo_resposta = True\n",
      "        elif linha:\n",
      "            if lendo_resposta:\n",
      "                resposta_atual += \" \" + linha\n",
      "            else:\n",
      "                falas_alunos.append(linha)\n",
      "    \n",
      "    if lendo_resposta and resposta_atual:\n",
      "        respostas_chatgpt.append(resposta_atual.strip())\n",
      "    \n",
      "    return falas_alunos, respostas_chatgpt\n",
      "\n",
      "# Texto de exemplo\n",
      "texto = \"\"\"\n",
      "oi\n",
      "ChatGPT\n",
      "Olá! Como posso ajudar hoje?\n",
      "\n",
      "bom dia\n",
      "ChatGPT\n",
      "Bom dia! Como está hoje? Posso te ajudar com alguma coisa?\n",
      "\"\"\"\n",
      "\n",
      "# Separar falas de alunos e respostas do ChatGPT\n",
      "falas_alunos, respostas_chatgpt = separar_falas(texto)\n",
      "\n",
      "print(\"Falas dos alunos:\")\n",
      "for fala in falas_alunos:\n",
      "    print(f\" - {fala}\")\n",
      "\n",
      "print(\"\\nRespostas do ChatGPT:\")\n",
      "for resposta in respostas_chatgpt:\n",
      "    print(f\" - {resposta}\")\n",
      "Explicação do Código\n",
      "Instalação de Dependências:\n",
      "Usamos comandos pip install spacy e python -m spacy download pt_core_news_sm para instalar spaCy e baixar o modelo de idioma em português.\n",
      "Carregamento do Modelo de Idioma:\n",
      "nlp = spacy.load('pt_core_news_sm'): Carrega o modelo de idioma em português.\n",
      "Função separar_falas:\n",
      "Inicialização das Listas: Inicializamos listas falas_alunos e respostas_chatgpt para armazenar as falas dos alunos e as respostas do ChatGPT, respectivamente.\n",
      "Divisão do Texto por Linhas: linhas = texto.split('\\n') divide o texto em linhas individuais.\n",
      "Controle de Resposta: Usamos uma variável lendo_resposta para indicar quando estamos lendo uma resposta do ChatGPT e resposta_atual para acumular a resposta.\n",
      "Processamento das Linhas: Iteramos sobre cada linha do texto.\n",
      "Se a linha for \"ChatGPT\", sinalizamos que começamos a ler uma resposta do ChatGPT.\n",
      "Se for uma linha de texto, adicionamos à resposta atual ou à lista de falas dos alunos, dependendo do estado de lendo_resposta.\n",
      "Finalização: Após a iteração, verificamos se há uma resposta atual não adicionada e a adicionamos à lista de respostas.\n",
      "Exemplo de Texto e Teste da Função:\n",
      "O exemplo de texto é processado e a função separar_falas é chamada para separar as falas dos alunos das respostas do ChatGPT.\n",
      "As falas e respostas são então impressas.\n",
      "Este algoritmo permite identificar e separar falas de alunos e respostas do ChatGPT, mesmo que o texto original não indique onde a fala do ChatGPT termina.\n",
      "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
      "Entity\n",
      "Date\n",
      "Seasonal variation\n",
      "Combined measurements\n",
      "Monthly averaged\n",
      "Annual averaged\n",
      "sea_temperature_anomaly\n",
      "Sea surface temp (lower-bound)\n",
      "Sea surface temp (upper-bound)\n",
      "Monthly pH measurement\n",
      "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
      "Análises Temporais\n",
      "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
      "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
      "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
      "Análises de Anomalias\n",
      "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
      "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
      "Análises Estatísticas\n",
      "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
      "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
      "Análises Comparativas\n",
      "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
      "Comparação sazonal: Comparar as medições sazonais ao longo de diferentes anos.\n",
      "Análises Espaciais (se os dados tiverem informações geográficas)\n",
      "Mapeamento de anomalias: Criar mapas para visualizar anomalias de temperatura e pH em diferentes regiões.\n",
      "Identificação de hotspots: Detectar áreas com mudanças climáticas significativas.\n",
      "Análises de Impacto\n",
      "Impacto ambiental: Estudar como as variações de temperatura e pH podem estar afetando a vida marinha e os ecossistemas.\n",
      "Impacto em atividades humanas: Analisar o impacto potencial das mudanças climáticas em atividades como a pesca e o turismo.\n",
      "Exemplos de Visualizações\n",
      "Gráficos de linha: Para mostrar tendências ao longo do tempo.\n",
      "Histogramas: Para visualizar a distribuição das temperaturas e pH.\n",
      "Mapas de calor: Para identificar regiões com mudanças significativas.\n",
      "Diagramas de dispersão: Para explorar correlações entre diferentes variáveis.\n",
      "Exemplos de Perguntas de Pesquisa\n",
      "Como a temperatura da superfície do mar mudou nos últimos 50 anos?\n",
      "Quais são as estações do ano com maiores variações de temperatura?\n",
      "Existe uma correlação entre a temperatura do mar e o pH em regiões específicas?\n",
      "Quais regiões apresentam as maiores anomalias de temperatura do mar?\n",
      "Como as anomalias de temperatura do mar estão afetando a biodiversidade marinha?\n",
      "Realizar essas análises pode fornecer insights valiosos sobre mudanças climáticas, variações sazonais e impactos ambientais, ajudando a informar políticas de mitigação e adaptação.\n"
     ]
    }
   ],
   "source": [
    "links = [\n",
    "    'https://chatgpt.com/share/dd647aa8-4909-41fc-a49f-9a7aa3e3052b',\n",
    "    'https://chatgpt.com/share/29dca0d3-6c89-4bb5-8a04-4efa4bdfbd90',\n",
    "\n",
    "]\n",
    "\n",
    "texts = extract_text_from_links(links)\n",
    "for text in texts:\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrair insights ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "def extrair_insights(texto):\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    # Definir categorias de insights que queremos identificar (posso escolher um dataset específico\n",
    "    # da cadeira e identificar quais os bons insights para aqueles dados)\n",
    "    categorias = {\n",
    "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
    "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
    "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
    "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
    "    }\n",
    "    \n",
    "    # Armazenar insights encontrados\n",
    "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
    "    \n",
    "    # Percorrer as sentenças do documento\n",
    "    for sent in doc.sents:\n",
    "        # Verificar se a sentença corresponde a algum insight nas categorias\n",
    "        for categoria, insights in categorias.items():\n",
    "            for insight in insights:\n",
    "                if insight.lower() in sent.text.lower():\n",
    "                    insights_encontrados[categoria].append(sent.text.strip())\n",
    "                    break\n",
    "    \n",
    "    return insights_encontrados\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separar Falas ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para separar falas de alunos e respostas do ChatGPT\n",
    "def separar_falas(texto):\n",
    "    falas_alunos = []\n",
    "    respostas_chatgpt = []\n",
    "    \n",
    "    linhas = texto.split('\\n')\n",
    "    \n",
    "    # Variável para controlar se estamos lendo uma resposta do ChatGPT\n",
    "    lendo_resposta = False\n",
    "    resposta_atual = \"\"\n",
    "    \n",
    "    for linha in linhas:\n",
    "        linha = linha.strip()\n",
    "        if linha == \"ChatGPT\":\n",
    "            if lendo_resposta and resposta_atual:\n",
    "                respostas_chatgpt.append(resposta_atual.strip())\n",
    "                resposta_atual = \"\"\n",
    "            lendo_resposta = True\n",
    "        elif linha:\n",
    "            if lendo_resposta:\n",
    "                resposta_atual += \" \" + linha\n",
    "            else:\n",
    "                falas_alunos.append(linha)\n",
    "    \n",
    "    if lendo_resposta and resposta_atual:\n",
    "        respostas_chatgpt.append(resposta_atual.strip())\n",
    "    \n",
    "    return falas_alunos, respostas_chatgpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex de texto grande com interação ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"\"\"\n",
    "Estou realizando um experimento, alunos vão interagir com o chatgpt, o texto da interação deles será exportado e analisado. O aluno irá utilizar o chatgpt para tentar resolver problemas de ciência de dados descritiva, a questão diz respeito a insights obtidos sobre uma base de dados, quero um algoritmo python de PLN que analise quais são os insights gerados pelo chatgpt neste texto, por exemplo, no texto gerado pelo chatgpt abaixo ele sugere 8 insights:\n",
    "\n",
    "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
    "Entity\n",
    "Date\n",
    "Seasonal variation\n",
    "Combined measurements\n",
    "Monthly averaged\n",
    "Annual averaged\n",
    "sea_temperature_anomaly\n",
    "Sea surface temp (lower-bound)\n",
    "Sea surface temp (upper-bound)\n",
    "Monthly pH measurement\n",
    "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
    "\n",
    "Análises Temporais\n",
    "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
    "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
    "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
    "Análises de Anomalias\n",
    "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
    "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
    "Análises Estatísticas\n",
    "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
    "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
    "Análises Comparativas\n",
    "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
    "ChatGPT\n",
    "Para criar um algoritmo em Python que analise textos gerados pelo ChatGPT e identifique insights, você pode usar técnicas de Processamento de Linguagem Natural (PLN) como a tokenização, identificação de palavras-chave e análise de padrões. O exemplo abaixo demonstra como você pode implementar isso usando a biblioteca spaCy e técnicas de PLN.\n",
    "\n",
    "Primeiro, você precisará instalar a biblioteca spaCy e baixar o modelo de idioma:\n",
    "\n",
    "bash\n",
    "Copiar código\n",
    "pip install spacy\n",
    "python -m spacy download pt_core_news_sm\n",
    "Em seguida, você pode usar o seguinte código Python para analisar os textos:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de idioma em português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Função para extrair insights do texto\n",
    "def extrair_insights(texto):\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    # Definir categorias de insights que queremos identificar\n",
    "    categorias = {\n",
    "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
    "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
    "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
    "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
    "    }\n",
    "    \n",
    "    # Armazenar insights encontrados\n",
    "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
    "    \n",
    "    # Percorrer as sentenças do documento\n",
    "    for sent in doc.sents:\n",
    "        # Verificar se a sentença corresponde a algum insight nas categorias\n",
    "        for categoria, insights in categorias.items():\n",
    "            for insight in insights:\n",
    "                if insight.lower() in sent.text.lower():\n",
    "                    insights_encontrados[categoria].append(sent.text.strip())\n",
    "                    break\n",
    "    \n",
    "    return insights_encontrados\n",
    "\n",
    "# Texto de exemplo fornecido pelo usuário\n",
    "texto = \"\"\n",
    "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
    "Entity\n",
    "Date\n",
    "Seasonal variation\n",
    "Combined measurements\n",
    "Monthly averaged\n",
    "Annual averaged\n",
    "sea_temperature_anomaly\n",
    "Sea surface temp (lower-bound)\n",
    "Sea surface temp (upper-bound)\n",
    "Monthly pH measurement\n",
    "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
    "\n",
    "Análises Temporais\n",
    "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
    "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
    "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
    "Análises de Anomalias\n",
    "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
    "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
    "Análises Estatísticas\n",
    "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
    "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
    "Análises Comparativas\n",
    "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
    "\"\"\n",
    "\n",
    "# Extrair e exibir os insights do texto\n",
    "insights = extrair_insights(texto)\n",
    "for categoria, sentencas in insights.items():\n",
    "    print(f\"{categoria}:\")\n",
    "    for sentenca in sentencas:\n",
    "        print(f\" - {sentenca}\")\n",
    "Explicação:\n",
    "Instalação de spaCy e download do modelo: Você precisa instalar a biblioteca spaCy e baixar o modelo de idioma português (pt_core_news_sm).\n",
    "\n",
    "Definição das categorias de insights: Criei um dicionário categorias que mapeia as diferentes categorias de análises e seus insights correspondentes.\n",
    "\n",
    "Processamento do texto: A função extrair_insights processa o texto usando spaCy, identifica sentenças que correspondem a cada insight e armazena os resultados.\n",
    "\n",
    "Extração e exibição dos insights: O script percorre o texto de exemplo fornecido e exibe os insights encontrados em cada categoria.\n",
    "\n",
    "Esse algoritmo pode ser expandido e adaptado conforme necessário para melhor atender às especificidades do seu experimento e análise.\n",
    "\n",
    "explique melhor o codigo que você gerou\n",
    "ChatGPT\n",
    "Claro, vou explicar detalhadamente cada parte do código e seu funcionamento.\n",
    "\n",
    "Instalação de dependências e download do modelo de idioma\n",
    "Primeiro, precisamos instalar a biblioteca spaCy e baixar o modelo de idioma português:\n",
    "\n",
    "bash\n",
    "Copiar código\n",
    "pip install spacy\n",
    "python -m spacy download pt_core_news_sm\n",
    "Importação da biblioteca e carregamento do modelo\n",
    "python\n",
    "Copiar código\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de idioma em português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "spacy: Importa a biblioteca spaCy que será utilizada para o processamento de linguagem natural.\n",
    "nlp: Carrega o modelo de idioma em português (pt_core_news_sm) que é necessário para processar o texto em português.\n",
    "Função para extrair insights do texto\n",
    "python\n",
    "Copiar código\n",
    "def extrair_insights(texto):\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    # Definir categorias de insights que queremos identificar\n",
    "    categorias = {\n",
    "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
    "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
    "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
    "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
    "    }\n",
    "    \n",
    "    # Armazenar insights encontrados\n",
    "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
    "    \n",
    "    # Percorrer as sentenças do documento\n",
    "    for sent in doc.sents:\n",
    "        # Verificar se a sentença corresponde a algum insight nas categorias\n",
    "        for categoria, insights in categorias.items():\n",
    "            for insight in insights:\n",
    "                if insight.lower() in sent.text.lower():\n",
    "                    insights_encontrados[categoria].append(sent.text.strip())\n",
    "                    break\n",
    "    \n",
    "    return insights_encontrados\n",
    "Processamento do texto com spaCy:\n",
    "\n",
    "doc = nlp(texto): Processa o texto fornecido usando o modelo nlp. Isso converte o texto em um objeto Doc que pode ser analisado em termos de sentenças, tokens e outros componentes linguísticos.\n",
    "Definição das categorias de insights:\n",
    "\n",
    "categorias: Um dicionário que mapeia categorias de análises para listas de possíveis insights que queremos identificar no texto. Cada chave é uma categoria e cada valor é uma lista de possíveis frases de insights.\n",
    "Inicialização da estrutura para armazenar os insights encontrados:\n",
    "\n",
    "insights_encontrados = {categoria: [] for categoria in categorias}: Inicializa um dicionário onde cada chave é uma categoria e cada valor é uma lista vazia que armazenará os insights encontrados.\n",
    "Percorrer as sentenças do documento:\n",
    "\n",
    "for sent in doc.sents: Itera sobre cada sentença do documento processado.\n",
    "Verificar se a sentença corresponde a algum insight nas categorias:\n",
    "\n",
    "Para cada sentença, verificamos se contém alguma das frases de insight definidas em categorias.\n",
    "if insight.lower() in sent.text.lower(): Verifica se a frase de insight aparece na sentença, ignorando diferenças de capitalização (letras maiúsculas e minúsculas).\n",
    "insights_encontrados[categoria].append(sent.text.strip()): Se a frase de insight é encontrada na sentença, a sentença é adicionada à lista correspondente no dicionário insights_encontrados.\n",
    "Retorna os insights encontrados:\n",
    "\n",
    "return insights_encontrados: A função retorna o dicionário insights_encontrados contendo as sentenças de insights agrupadas por categoria.\n",
    "Texto de exemplo fornecido pelo usuário\n",
    "python\n",
    "Copiar código\n",
    "# Texto de exemplo fornecido pelo usuário\n",
    "texto = \"\"\n",
    "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
    "Entity\n",
    "Date\n",
    "Seasonal variation\n",
    "Combined measurements\n",
    "Monthly averaged\n",
    "Annual averaged\n",
    "sea_temperature_anomaly\n",
    "Sea surface temp (lower-bound)\n",
    "Sea surface temp (upper-bound)\n",
    "Monthly pH measurement\n",
    "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
    "\n",
    "Análises Temporais\n",
    "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
    "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
    "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
    "Análises de Anomalias\n",
    "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
    "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
    "Análises Estatísticas\n",
    "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
    "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
    "Análises Comparativas\n",
    "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
    "\"\"\n",
    "Extração e exibição dos insights\n",
    "python\n",
    "Copiar código\n",
    "# Extrair e exibir os insights do texto\n",
    "insights = extrair_insights(texto)\n",
    "for categoria, sentencas in insights.items():\n",
    "    print(f\"{categoria}:\")\n",
    "    for sentenca in sentencas:\n",
    "        print(f\" - {sentenca}\")\n",
    "Extrair insights do texto:\n",
    "\n",
    "insights = extrair_insights(texto): Chama a função extrair_insights com o texto de exemplo e armazena o resultado no dicionário insights.\n",
    "Exibir os insights extraídos:\n",
    "\n",
    "for categoria, sentencas in insights.items(): Itera sobre cada categoria de insights e suas sentenças correspondentes.\n",
    "print(f\"{categoria}:\"): Imprime o nome da categoria.\n",
    "for sentenca in sentencas: Itera sobre cada sentença de insight na categoria.\n",
    "print(f\" - {sentenca}\"): Imprime cada sentença de insight, precedida por um traço para facilitar a leitura.\n",
    "Resumo\n",
    "O código processa um texto usando técnicas de PLN para identificar e extrair insights específicos definidos em categorias predefinidas. Ele utiliza o modelo de linguagem spaCy para análise e processamento, verifica a presença de frases de insight no texto e organiza os resultados por categoria. A abordagem é modular e pode ser adaptada para diferentes contextos e necessidades de análise.\n",
    "\n",
    "lembre-se que o código sera utilizado em um notebook no google colab\n",
    "ChatGPT\n",
    "Entendi, vamos adaptar o código para que funcione de forma eficaz no Google Colab. Aqui está o código completo, incluindo a instalação das dependências diretamente no notebook:\n",
    "\n",
    "Passo 1: Instalação das Dependências\n",
    "No Google Colab, você pode instalar as dependências diretamente em uma célula do notebook:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "Passo 2: Código Completo para Análise de Insights\n",
    "Em seguida, você pode criar uma célula com o código para analisar os textos e extrair insights:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de idioma em português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Função para extrair insights do texto\n",
    "def extrair_insights(texto):\n",
    "    # Processar o texto com spaCy\n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    # Definir categorias de insights que queremos identificar\n",
    "    categorias = {\n",
    "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
    "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
    "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
    "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
    "    }\n",
    "    \n",
    "    # Armazenar insights encontrados\n",
    "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
    "    \n",
    "    # Percorrer as sentenças do documento\n",
    "    for sent in doc.sents:\n",
    "        # Verificar se a sentença corresponde a algum insight nas categorias\n",
    "        for categoria, insights in categorias.items():\n",
    "            for insight in insights:\n",
    "                if insight.lower() in sent.text.lower():\n",
    "                    insights_encontrados[categoria].append(sent.text.strip())\n",
    "                    break\n",
    "    \n",
    "    return insights_encontrados\n",
    "\n",
    "# Texto de exemplo fornecido pelo usuário\n",
    "texto = \"\"\n",
    "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
    "Entity\n",
    "Date\n",
    "Seasonal variation\n",
    "Combined measurements\n",
    "Monthly averaged\n",
    "Annual averaged\n",
    "sea_temperature_anomaly\n",
    "Sea surface temp (lower-bound)\n",
    "Sea surface temp (upper-bound)\n",
    "Monthly pH measurement\n",
    "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
    "\n",
    "Análises Temporais\n",
    "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
    "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
    "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
    "Análises de Anomalias\n",
    "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
    "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
    "Análises Estatísticas\n",
    "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
    "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
    "Análises Comparativas\n",
    "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
    "\"\"\n",
    "\n",
    "# Extrair e exibir os insights do texto\n",
    "insights = extrair_insights(texto)\n",
    "for categoria, sentencas in insights.items():\n",
    "    print(f\"{categoria}:\")\n",
    "    for sentenca in sentencas:\n",
    "        print(f\" - {sentenca}\")\n",
    "Explicação\n",
    "Instalação das Dependências:\n",
    "\n",
    "As dependências são instaladas diretamente em uma célula do Google Colab usando !pip install spacy e !python -m spacy download pt_core_news_sm.\n",
    "Importação e Carregamento do Modelo:\n",
    "\n",
    "O modelo de linguagem spaCy é carregado com nlp = spacy.load('pt_core_news_sm').\n",
    "Definição da Função extrair_insights:\n",
    "\n",
    "A função extrair_insights processa o texto e verifica a presença de frases de insight predefinidas, organizando os resultados por categoria.\n",
    "Texto de Exemplo e Extração de Insights:\n",
    "\n",
    "Um texto de exemplo é fornecido e a função extrair_insights é chamada para analisar o texto e extrair os insights.\n",
    "Os insights são então exibidos por categoria.\n",
    "Este código pode ser executado em um notebook do Google Colab e permite que você analise textos para extrair insights específicos definidos nas categorias de análise.\n",
    "\n",
    "explique melhor a biblioteca spacy\n",
    "ChatGPT\n",
    "Claro! Vamos explorar a biblioteca spaCy e entender como ela funciona, incluindo suas principais funcionalidades e como usá-la para Processamento de Linguagem Natural (PLN).\n",
    "\n",
    "O que é spaCy?\n",
    "spaCy é uma biblioteca de código aberto para processamento de linguagem natural (PLN) avançado em Python. Ela é projetada para ser rápida, eficiente e capaz de lidar com grandes volumes de texto. spaCy fornece ferramentas para várias tarefas de PLN, como tokenização, análise morfológica, extração de entidades nomeadas e muito mais.\n",
    "\n",
    "Funcionalidades Principais de spaCy\n",
    "Tokenização: Dividir o texto em palavras, pontuações e outros elementos chamados \"tokens\".\n",
    "POS Tagging (Part-of-Speech Tagging): Identificar a função gramatical de cada token (substantivo, verbo, adjetivo, etc.).\n",
    "Parsing: Analisar a estrutura gramatical do texto, identificando relações entre as palavras.\n",
    "NER (Named Entity Recognition): Identificar e classificar entidades nomeadas (pessoas, organizações, locais, etc.) no texto.\n",
    "Vetores de Palavra (Word Vectors): Representar palavras como vetores de números para análise semântica.\n",
    "Lemmatização: Reduzir palavras às suas formas básicas ou radicais.\n",
    "Similaridade de Texto: Calcular a similaridade entre diferentes textos.\n",
    "Como usar spaCy\n",
    "Instalação\n",
    "Para instalar spaCy e um modelo de idioma, você pode usar os seguintes comandos:\n",
    "\n",
    "bash\n",
    "Copiar código\n",
    "pip install spacy\n",
    "python -m spacy download pt_core_news_sm\n",
    "Carregamento de um Modelo de Idioma\n",
    "Após a instalação, você pode carregar um modelo de idioma:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de idioma em português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "Processamento de Texto\n",
    "Com o modelo carregado, você pode processar texto:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "# Texto de exemplo\n",
    "texto = \"A análise de dados é essencial para entender padrões climáticos.\"\n",
    "\n",
    "# Processar o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Iterar sobre tokens no documento\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)\n",
    "token.text: O texto do token.\n",
    "token.pos_: A parte do discurso (part-of-speech) do token.\n",
    "token.dep_: A relação de dependência gramatical do token.\n",
    "Extração de Entidades Nomeadas\n",
    "python\n",
    "Copiar código\n",
    "# Texto de exemplo com entidades nomeadas\n",
    "texto = \"A análise de dados realizada pela Universidade de São Paulo é essencial.\"\n",
    "\n",
    "# Processar o texto\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Iterar sobre entidades no documento\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "ent.text: O texto da entidade nomeada.\n",
    "ent.label_: O rótulo da entidade (e.g., organização, pessoa, localização).\n",
    "Similaridade de Texto\n",
    "Você pode comparar a similaridade entre documentos ou frases:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "doc1 = nlp(\"A análise de dados é essencial.\")\n",
    "doc2 = nlp(\"A estatística é uma ferramenta fundamental para a ciência de dados.\")\n",
    "\n",
    "# Calcular similaridade\n",
    "similaridade = doc1.similarity(doc2)\n",
    "print(f\"Similaridade: {similaridade}\")\n",
    "Aplicação: Extração de Insights\n",
    "Usando as funcionalidades de spaCy, você pode criar funções para tarefas específicas, como a extração de insights que mostramos anteriormente. spaCy permite analisar texto de forma detalhada e extrair informações relevantes com facilidade.\n",
    "\n",
    "Exemplo Completo no Google Colab\n",
    "python\n",
    "Copiar código\n",
    "# Passo 1: Instalação das Dependências\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "\n",
    "# Passo 2: Importar e Carregar o Modelo\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Função para extrair insights do texto\n",
    "def extrair_insights(texto):\n",
    "    doc = nlp(texto)\n",
    "    categorias = {\n",
    "        \"Análises Temporais\": [\"Tendências de longo prazo\", \"Análise sazonal\", \"Variações mensais\"],\n",
    "        \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"],\n",
    "        \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"Correlação entre variáveis\"],\n",
    "        \"Análises Comparativas\": [\"Comparação anual e mensal\"]\n",
    "    }\n",
    "    insights_encontrados = {categoria: [] for categoria in categorias}\n",
    "    \n",
    "    for sent in doc.sents:\n",
    "        for categoria, insights in categorias.items():\n",
    "            for insight in insights:\n",
    "                if insight.lower() in sent.text.lower():\n",
    "                    insights_encontrados[categoria].append(sent.text.strip())\n",
    "                    break\n",
    "    \n",
    "    return insights_encontrados\n",
    "\n",
    "# Texto de exemplo fornecido pelo usuário\n",
    "texto = \"\"\n",
    "quais os tipos de analises e informações que posso obter a partir de uma base de dados sobre clima com as seguintes informações:\n",
    "Entity\n",
    "Date\n",
    "Seasonal variation\n",
    "Combined measurements\n",
    "Monthly averaged\n",
    "Annual averaged\n",
    "sea_temperature_anomaly\n",
    "Sea surface temp (lower-bound)\n",
    "Sea surface temp (upper-bound)\n",
    "Monthly pH measurement\n",
    "Com uma base de dados climáticos contendo as informações que você mencionou, você pode realizar uma variedade de análises e obter diferentes tipos de informações. Aqui estão algumas sugestões:\n",
    "\n",
    "Análises Temporais\n",
    "Tendências de longo prazo: Analisar as tendências de aquecimento ou resfriamento ao longo dos anos.\n",
    "Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
    "Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
    "Análises de Anomalias\n",
    "Anomalias de temperatura do mar: Identificar períodos e regiões com desvios significativos da média histórica.\n",
    "Anomalias de pH: Detectar mudanças nos níveis de acidez do oceano ao longo do tempo.\n",
    "Análises Estatísticas\n",
    "Distribuição de temperaturas: Analisar a distribuição das temperaturas da superfície do mar, usando os limites inferior e superior.\n",
    "Correlação entre variáveis: Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH.\n",
    "Análises Comparativas\n",
    "Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n",
    "\"\"\n",
    "\n",
    "# Extrair e exibir os insights do texto\n",
    "insights = extrair_insights(texto)\n",
    "for categoria, sentencas in insights.items():\n",
    "    print(f\"{categoria}:\")\n",
    "    for sentenca in sentencas:\n",
    "        print(f\" - {sentenca}\")\n",
    "Conclusão\n",
    "A biblioteca spaCy é uma ferramenta poderosa para PLN, permitindo análise e processamento avançados de texto. Com suas diversas funcionalidades, spaCy facilita a extração de insights e informações relevantes de grandes volumes de texto, sendo uma excelente escolha para projetos de ciência de dados e análise de texto.\n",
    "\n",
    "eu quero que o algoritmo consiga identificar quais as falas geradas por alunos e quais as respostas fornecidas pelo chatgpt, por exemplo no texto abaixo existem marcações de onde a fala do chatgpt começa, mas não existe indicação de onde ela termina:\n",
    "\n",
    "oi\n",
    "ChatGPT\n",
    "Olá! Como posso ajudar hoje?\n",
    "\n",
    "bom dia\n",
    "ChatGPT\n",
    "Bom dia! Como está hoje? Posso te ajudar com alguma coisa?\n",
    "ChatGPT\n",
    "Para criar um algoritmo que consiga identificar quais são as falas geradas por alunos e quais são as respostas fornecidas pelo ChatGPT, podemos usar a estrutura do texto e as marcações fornecidas. Vamos desenvolver um código que faça essa separação.\n",
    "\n",
    "Passo 1: Instalação das Dependências\n",
    "Primeiro, vamos garantir que temos o spaCy instalado e o modelo de idioma em português baixado, caso ainda não tenha sido feito:\n",
    "\n",
    "bash\n",
    "Copiar código\n",
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "Passo 2: Função para Separar Falas\n",
    "Vamos criar uma função que separa as falas dos alunos e as respostas do ChatGPT:\n",
    "\n",
    "python\n",
    "Copiar código\n",
    "import spacy\n",
    "\n",
    "# Carregar o modelo de idioma em português\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "# Função para separar falas de alunos e respostas do ChatGPT\n",
    "def separar_falas(texto):\n",
    "    # Inicializar listas para armazenar falas de alunos e respostas do ChatGPT\n",
    "    falas_alunos = []\n",
    "    respostas_chatgpt = []\n",
    "    \n",
    "    # Dividir o texto por linhas\n",
    "    linhas = texto.split('\\n')\n",
    "    \n",
    "    # Variável para controlar se estamos lendo uma resposta do ChatGPT\n",
    "    lendo_resposta = False\n",
    "    resposta_atual = \"\"\n",
    "    \n",
    "    for linha in linhas:\n",
    "        linha = linha.strip()\n",
    "        if linha == \"ChatGPT\":\n",
    "            if lendo_resposta and resposta_atual:\n",
    "                respostas_chatgpt.append(resposta_atual.strip())\n",
    "                resposta_atual = \"\"\n",
    "            lendo_resposta = True\n",
    "        elif linha:\n",
    "            if lendo_resposta:\n",
    "                resposta_atual += \" \" + linha\n",
    "            else:\n",
    "                falas_alunos.append(linha)\n",
    "    \n",
    "    if lendo_resposta and resposta_atual:\n",
    "        respostas_chatgpt.append(resposta_atual.strip())\n",
    "    \n",
    "    return falas_alunos, respostas_chatgpt\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"\"\n",
    "oi\n",
    "ChatGPT\n",
    "Olá! Como posso ajudar hoje?\n",
    "\n",
    "bom dia\n",
    "ChatGPT\n",
    "Bom dia! Como está hoje? Posso te ajudar com alguma coisa?\n",
    "\"\"\n",
    "\n",
    "# Separar falas de alunos e respostas do ChatGPT\n",
    "falas_alunos, respostas_chatgpt = separar_falas(texto)\n",
    "\n",
    "print(\"Falas dos alunos:\")\n",
    "for fala in falas_alunos:\n",
    "    print(f\" - {fala}\")\n",
    "\n",
    "print(\"\\nRespostas do ChatGPT:\")\n",
    "for resposta in respostas_chatgpt:\n",
    "    print(f\" - {resposta}\")\n",
    "Explicação do Código\n",
    "Instalação de Dependências:\n",
    "\n",
    "Usamos comandos pip install spacy e python -m spacy download pt_core_news_sm para instalar spaCy e baixar o modelo de idioma em português.\n",
    "Carregamento do Modelo de Idioma:\n",
    "\n",
    "nlp = spacy.load('pt_core_news_sm'): Carrega o modelo de idioma em português.\n",
    "Função separar_falas:\n",
    "\n",
    "Inicialização das Listas: Inicializamos listas falas_alunos e respostas_chatgpt para armazenar as falas dos alunos e as respostas do ChatGPT, respectivamente.\n",
    "Divisão do Texto por Linhas: linhas = texto.split('\\n') divide o texto em linhas individuais.\n",
    "Controle de Resposta: Usamos uma variável lendo_resposta para indicar quando estamos lendo uma resposta do ChatGPT e resposta_atual para acumular a resposta.\n",
    "Processamento das Linhas: Iteramos sobre cada linha do texto.\n",
    "Se a linha for \"ChatGPT\", sinalizamos que começamos a ler uma resposta do ChatGPT.\n",
    "Se for uma linha de texto, adicionamos à resposta atual ou à lista de falas dos alunos, dependendo do estado de lendo_resposta.\n",
    "Finalização: Após a iteração, verificamos se há uma resposta atual não adicionada e a adicionamos à lista de respostas.\n",
    "Exemplo de Texto e Teste da Função:\n",
    "\n",
    "O exemplo de texto é processado e a função separar_falas é chamada para separar as falas dos alunos das respostas do ChatGPT.\n",
    "As falas e respostas são então impressas.\n",
    "Este algoritmo permite identificar e separar falas de alunos e respostas do ChatGPT, mesmo que o texto original não indique onde a fala do ChatGPT termina.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação de falas na interação ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "falas_alunos, respostas_chatgpt = separar_falas(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoria: Análises Temporais:\n",
      "Sentença - Definir categorias de insights que queremos identificar categorias = { \"Análises Temporais\": [\"Tendências de longo prazo\", \"\n",
      "Sentença - Análise sazonal\", \"Variações mensais\"]\n",
      "Sentença - Análises Temporais Tendências de longo prazo:\n",
      "Sentença - Análise sazonal: Verificar como as medições variam de acordo com as estações do ano.\n",
      "Sentença - Variações mensais: Identificar padrões e variações mensais, como a diferença entre os meses de verão e inverno.\n",
      "categoria: Análises de Anomalias:\n",
      "Sentença - , \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"], \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"\n",
      "Sentença - Análises de Anomalias Anomalias de temperatura do mar:\n",
      "Sentença - Anomalias de pH:\n",
      "categoria: Análises Estatísticas:\n",
      "Sentença - , \"Análises de Anomalias\": [\"Anomalias de temperatura do mar\", \"Anomalias de pH\"], \"Análises Estatísticas\": [\"Distribuição de temperaturas\", \"\n",
      "Sentença - Correlação entre variáveis\"]\n",
      "Sentença - Análises Estatísticas Distribuição de temperaturas:\n",
      "Sentença - Correlação entre variáveis:\n",
      "categoria: Análises Comparativas:\n",
      "Sentença - , \"Análises Comparativas\": [\"Comparação anual e mensal\"] } #\n",
      "Sentença - Investigar correlações entre diferentes variáveis, como a temperatura da superfície do mar e o pH. Análises Comparativas Comparação anual e mensal: Comparar médias mensais e anuais para identificar mudanças e padrões.\n"
     ]
    }
   ],
   "source": [
    "insights = extrair_insights(respostas_chatgpt[0])\n",
    "for categoria, sentencas in insights.items():\n",
    "    print(f\"categoria: {categoria}:\")\n",
    "    for sentenca in sentencas:\n",
    "        print(f\"Sentença - {sentenca}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
